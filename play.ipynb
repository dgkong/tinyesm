{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448e445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekong/miniforge3/envs/tinyesm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm.embeddings.word_embeddings.weight torch.Size([33, 320])\n",
      "esm.embeddings.position_embeddings.weight torch.Size([1026, 320])\n",
      "esm.encoder.layer.0.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.0.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.0.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.0.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.0.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.0.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.0.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.0.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.0.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.1.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.1.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.1.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.1.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.1.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.1.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.1.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.1.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.2.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.2.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.2.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.2.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.2.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.2.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.2.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.2.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.3.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.3.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.3.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.3.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.3.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.3.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.3.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.3.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.4.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.4.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.4.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.4.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.4.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.4.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.4.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.4.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.5.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.5.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.5.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.5.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.5.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.5.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.5.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.5.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.emb_layer_norm_after.weight torch.Size([320])\n",
      "esm.encoder.emb_layer_norm_after.bias torch.Size([320])\n",
      "esm.contact_head.regression.weight torch.Size([1, 120])\n",
      "esm.contact_head.regression.bias torch.Size([1])\n",
      "lm_head.bias torch.Size([33])\n",
      "lm_head.dense.weight torch.Size([320, 320])\n",
      "lm_head.dense.bias torch.Size([320])\n",
      "lm_head.layer_norm.weight torch.Size([320])\n",
      "lm_head.layer_norm.bias torch.Size([320])\n",
      "lm_head.decoder.weight torch.Size([33, 320])\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmForMaskedLM\n",
    "\n",
    "esm = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "esm_sd = esm.state_dict()\n",
    "\n",
    "for k, v in esm_sd.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc8954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import EsmTokenizer\n",
    "\n",
    "from data import ShardedMLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39dd5942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 122]) torch.Size([33, 122]) torch.Size([33, 122])\n",
      "4026\n"
     ]
    }
   ],
   "source": [
    "T = 510\n",
    "TOKENS_PER_BATCH = 4096\n",
    "\n",
    "train_loader = DataLoader(ShardedMLMDataset(crop_len=T, tokens_per_batch=TOKENS_PER_BATCH, split='train'), batch_size=None)\n",
    "item = next(iter(train_loader))\n",
    "print(item[0].shape, item[1].shape, item[2].shape)\n",
    "print(item[0].numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a071b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "{'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
      "<cls> M I Q Q T L L L Y G Y P F G T <unk> V L K D N L <mask> V G Q M R T <mask> R S I D Q <mask> <mask> S R E W M V D <mask> C T E W L A I V T F S P G V C R K E S Q T <null_1> F C S <mask> I G W C P K N E S <mask> <mask> I L A G G Q A K T T L E G S S L F P S I <mask> G L E N C R <mask> E F T E S T N L R E <eos>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> L <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> D <unk> <unk> <unk> <unk> <unk> <unk> F <unk> <unk> <unk> <unk> <unk> <unk> Q <unk> <unk> <unk> <unk> <unk> Y E <unk> <unk> <unk> <unk> <unk> <unk> <unk> R <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> L <unk> <unk> <unk> I <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> R R <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> S <unk> <unk> <unk> <unk> <unk> <unk> E <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.get_vocab())\n",
    "example_seq = tokenizer.decode(item[0][0])\n",
    "example_label = tokenizer.decode(item[1][0])\n",
    "example_mask = ','.join(map(str, item[2][0].tolist()))\n",
    "print(example_seq)\n",
    "print(example_label)\n",
    "print(example_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b703549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "step 0 | esm_loss: 2.3216257095336914\n",
      "step 0 | fair_esm_loss: 2.321367025375366\n",
      "step 10 | esm_loss: 2.06491756439209\n",
      "step 10 | fair_esm_loss: 2.0659947395324707\n",
      "step 20 | esm_loss: 2.179452896118164\n",
      "step 20 | fair_esm_loss: 2.1791834831237793\n",
      "step 30 | esm_loss: 2.168743133544922\n",
      "step 30 | fair_esm_loss: 2.1677305698394775\n",
      "step 40 | esm_loss: 2.153233766555786\n",
      "step 40 | fair_esm_loss: 2.15655255317688\n"
     ]
    }
   ],
   "source": [
    "from model import ESM, ESMConfig\n",
    "from transformers import EsmForMaskedLM\n",
    "import esm\n",
    "\n",
    "def generate_sequence(logits):\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "esm_model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "esm_model.to(\"cpu\")\n",
    "esm_model.eval()\n",
    "for p in esm_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "fair_esm_model, _ = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "fair_esm_model.to(\"cpu\")\n",
    "fair_esm_model.eval()\n",
    "for p in fair_esm_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "model = ESM(ESMConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs, labels, mask = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # logits, loss = model(inputs, mask, labels)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        inputs = inputs.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "        mask = mask.to(\"cpu\")\n",
    "        esm_output = esm_model(input_ids=inputs, attention_mask=mask, labels=labels)\n",
    "        esm_logits = esm_output.logits\n",
    "        esm_loss = esm_output.loss\n",
    "        fair_esm_logits = fair_esm_model(inputs, repr_layers=[], return_contacts=False)[\"logits\"]\n",
    "        fair_esm_loss = F.cross_entropy(fair_esm_logits.view(-1, fair_esm_logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "        # print(f\"step {i} | loss: {loss.detach().item()}\")\n",
    "        print(f\"step {i} | esm_loss: {esm_loss.detach().item()}\")\n",
    "        print(f\"step {i} | fair_esm_loss: {fair_esm_loss.detach().item()}\")\n",
    "        # print(f\"step {i} | lab ==> {tokenizer.decode(labels[0][labels[0] != -100])}\")\n",
    "        # print(f\"step {i} | gen ==> {' '.join(generate_sequence(logits[0][labels[0] != -100]))}\")\n",
    "        # print(f\"step {i} | esm_gen ==> {' '.join(generate_sequence(esm_logits[0][labels[0] != -100]))}\")\n",
    "    \n",
    "    if i >= 49:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700f081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyesm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
