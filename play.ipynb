{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448e445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm.embeddings.word_embeddings.weight torch.Size([33, 320])\n",
      "esm.embeddings.position_embeddings.weight torch.Size([1026, 320])\n",
      "esm.encoder.layer.0.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.0.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.0.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.0.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.0.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.0.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.0.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.0.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.0.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.0.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.0.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.1.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.1.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.1.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.1.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.1.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.1.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.1.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.1.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.1.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.1.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.2.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.2.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.2.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.2.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.2.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.2.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.2.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.2.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.2.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.2.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.3.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.3.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.3.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.3.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.3.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.3.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.3.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.3.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.3.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.3.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.4.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.4.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.4.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.4.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.4.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.4.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.4.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.4.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.4.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.4.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.query.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.self.query.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.key.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.self.key.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.value.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.self.value.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq torch.Size([8])\n",
      "esm.encoder.layer.5.attention.output.dense.weight torch.Size([320, 320])\n",
      "esm.encoder.layer.5.attention.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.5.attention.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.5.attention.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.layer.5.intermediate.dense.weight torch.Size([1280, 320])\n",
      "esm.encoder.layer.5.intermediate.dense.bias torch.Size([1280])\n",
      "esm.encoder.layer.5.output.dense.weight torch.Size([320, 1280])\n",
      "esm.encoder.layer.5.output.dense.bias torch.Size([320])\n",
      "esm.encoder.layer.5.LayerNorm.weight torch.Size([320])\n",
      "esm.encoder.layer.5.LayerNorm.bias torch.Size([320])\n",
      "esm.encoder.emb_layer_norm_after.weight torch.Size([320])\n",
      "esm.encoder.emb_layer_norm_after.bias torch.Size([320])\n",
      "esm.contact_head.regression.weight torch.Size([1, 120])\n",
      "esm.contact_head.regression.bias torch.Size([1])\n",
      "lm_head.bias torch.Size([33])\n",
      "lm_head.dense.weight torch.Size([320, 320])\n",
      "lm_head.dense.bias torch.Size([320])\n",
      "lm_head.layer_norm.weight torch.Size([320])\n",
      "lm_head.layer_norm.bias torch.Size([320])\n",
      "lm_head.decoder.weight torch.Size([33, 320])\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmForMaskedLM\n",
    "\n",
    "esm = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "esm_sd = esm.state_dict()\n",
    "\n",
    "for k, v in esm_sd.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc8954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddiekong/miniforge3/envs/tinyesm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import EsmTokenizer\n",
    "\n",
    "from data import ShardedMLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39dd5942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 87]) torch.Size([47, 87]) torch.Size([47, 87])\n",
      "4089\n"
     ]
    }
   ],
   "source": [
    "T = 510\n",
    "TOKENS_PER_BATCH = 4096\n",
    "\n",
    "train_loader = DataLoader(ShardedMLMDataset(crop_len=T, tokens_per_batch=TOKENS_PER_BATCH, split='train'), batch_size=None)\n",
    "item = next(iter(train_loader))\n",
    "print(item[0].shape, item[1].shape, item[2].shape)\n",
    "print(item[0].numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a071b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "{'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
      "<cls> M R E <mask> Q L <mask> <mask> Y I K I H K L F L L K K <mask> E V V S <mask> E H E R F Y C Y F D <mask> <mask> <mask> T T I K G T T S K U V D <mask> K <mask> I W V E A R P E I R K <mask> D I D <mask> N P S G F I N A G E L <mask> P E <mask> <eos>\n",
      "<unk> <unk> <unk> <unk> K <unk> <unk> M E <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> M <unk> <unk> <unk> <unk> D <unk> <unk> <unk> S <unk> <unk> <unk> <unk> <unk> <unk> I N Y <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> I <unk> <unk> N <unk> V <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> K <unk> <unk> <unk> S <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> P <unk> <unk> K <unk>\n",
      "1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.get_vocab())\n",
    "example_seq = tokenizer.decode(item[0][0])\n",
    "example_label = tokenizer.decode(item[1][0])\n",
    "example_mask = ','.join(map(str, item[2][0].tolist()))\n",
    "print(example_seq)\n",
    "print(example_label)\n",
    "print(example_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b703549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "step 0 | loss: 3.5479190349578857\n",
      "step 0 | esm_loss: 2.491105318069458\n",
      "step 0 | lab ==> L E V D T S R K A\n",
      "step 0 | gen ==> Z Z Z Z Z Z Z I Z\n",
      "step 0 | esm_gen ==> D K E K K I I K L\n",
      "step 10 | loss: 2.9885172843933105\n",
      "step 10 | esm_loss: 2.318218231201172\n",
      "step 10 | lab ==> F S L S L F E W G R\n",
      "step 10 | gen ==> L L L L L L L L L L\n",
      "step 10 | esm_gen ==> F L L L L L L L K K\n",
      "step 20 | loss: 2.9062085151672363\n",
      "step 20 | esm_loss: 2.2791340351104736\n",
      "step 20 | lab ==> L L L Y H V I S A R H\n",
      "step 20 | gen ==> L L L L L L L L L L L\n",
      "step 20 | esm_gen ==> L L P L H P I L L R R\n",
      "step 30 | loss: 2.9326579570770264\n",
      "step 30 | esm_loss: 2.427870512008667\n",
      "step 30 | lab ==> I G Y S G R I K W K\n",
      "step 30 | gen ==> L L L L L L L L L L\n",
      "step 30 | esm_gen ==> I G G G L K A Y K K\n",
      "step 40 | loss: 2.944211483001709\n",
      "step 40 | esm_loss: 2.4567716121673584\n",
      "step 40 | lab ==> A I P I R S P S G\n",
      "step 40 | gen ==> A A A A A A A A A\n",
      "step 40 | esm_gen ==> A A A I A S A S S\n"
     ]
    }
   ],
   "source": [
    "from model import ESM, ESMConfig\n",
    "from transformers import EsmForMaskedLM\n",
    "\n",
    "def generate_sequence(logits):\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "esm_model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "esm_model.to(\"cpu\")\n",
    "esm_model.eval()\n",
    "for p in esm_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "model = ESM(ESMConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs, labels, mask = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(inputs, mask, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        inputs = inputs.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "        mask = mask.to(\"cpu\")\n",
    "        esm_output = esm_model(input_ids=inputs, attention_mask=mask, labels=labels)\n",
    "        esm_logits = esm_output.logits\n",
    "        esm_loss = esm_output.loss\n",
    "        print(f\"step {i} | loss: {loss.detach().item()}\")\n",
    "        print(f\"step {i} | esm_loss: {esm_loss.detach().item()}\")\n",
    "        print(f\"step {i} | lab ==> {tokenizer.decode(labels[0][labels[0] != -100])}\")\n",
    "        print(f\"step {i} | gen ==> {' '.join(generate_sequence(logits[0][labels[0] != -100]))}\")\n",
    "        print(f\"step {i} | esm_gen ==> {' '.join(generate_sequence(esm_logits[0][labels[0] != -100]))}\")\n",
    "    \n",
    "    if i >= 49:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d6e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyesm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
